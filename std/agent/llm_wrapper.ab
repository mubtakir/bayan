// ═══════════════════════════════════════════════════════════════════════════
// LLM Wrapper Module - Unified Interface for Language Models
// ═══════════════════════════════════════════════════════════════════════════
// 
// This module provides a unified interface for integrating with Ollama
// and other language models. It handles:
// - Model configuration and initialization
// - Prompt sending and response parsing
// - Error handling and fallback mechanisms
// - Performance monitoring and caching
//
// ═══════════════════════════════════════════════════════════════════════════

use std::collections::HashMap;

// ═══════════════════════════════════════════════════════════════════════════
// Data Structures
// ═══════════════════════════════════════════════════════════════════════════

pub struct OllamaConfig {
    pub model_name: String,
    pub base_url: String,
    pub timeout: i32,
    pub max_tokens: i32,
    pub temperature: f32,
    pub top_p: f32,
}

pub struct LLMResponse {
    pub text: String,
    pub model: String,
    pub tokens_used: i32,
    pub response_time: i32,
    pub success: bool,
    pub error_message: String,
}

pub struct LLMCache {
    pub cache: HashMap<String, LLMResponse>,
    pub max_size: i32,
    pub hit_count: i32,
    pub miss_count: i32,
}

pub struct LLMMetrics {
    pub total_requests: i32,
    pub successful_requests: i32,
    pub failed_requests: i32,
    pub average_response_time: i32,
    pub total_tokens_used: i32,
    pub cache_hit_rate: f32,
}

// ═══════════════════════════════════════════════════════════════════════════
// Configuration Functions
// ═══════════════════════════════════════════════════════════════════════════

pub fn create_ollama_config(
    model_name: String,
    base_url: String,
    timeout: i32,
    max_tokens: i32,
) -> OllamaConfig {
    OllamaConfig {
        model_name: model_name,
        base_url: base_url,
        timeout: timeout,
        max_tokens: max_tokens,
        temperature: 0.7,
        top_p: 0.9,
    }
}

pub fn create_default_config() -> OllamaConfig {
    OllamaConfig {
        model_name: "mistral".to_string(),
        base_url: "http://localhost:11434".to_string(),
        timeout: 30000,
        max_tokens: 2048,
        temperature: 0.7,
        top_p: 0.9,
    }
}

pub fn create_arabic_config() -> OllamaConfig {
    OllamaConfig {
        model_name: "aralama".to_string(),
        base_url: "http://localhost:11434".to_string(),
        timeout: 30000,
        max_tokens: 2048,
        temperature: 0.7,
        top_p: 0.9,
    }
}

// ═══════════════════════════════════════════════════════════════════════════
// Response Functions
// ═══════════════════════════════════════════════════════════════════════════

pub fn create_llm_response(
    text: String,
    model: String,
    tokens_used: i32,
    response_time: i32,
) -> LLMResponse {
    LLMResponse {
        text: text,
        model: model,
        tokens_used: tokens_used,
        response_time: response_time,
        success: true,
        error_message: "".to_string(),
    }
}

pub fn create_error_response(
    model: String,
    error_message: String,
) -> LLMResponse {
    LLMResponse {
        text: "".to_string(),
        model: model,
        tokens_used: 0,
        response_time: 0,
        success: false,
        error_message: error_message,
    }
}

// ═══════════════════════════════════════════════════════════════════════════
// Cache Functions
// ═══════════════════════════════════════════════════════════════════════════

pub fn create_llm_cache(max_size: i32) -> LLMCache {
    LLMCache {
        cache: HashMap::new(),
        max_size: max_size,
        hit_count: 0,
        miss_count: 0,
    }
}

pub fn cache_get(cache: &mut LLMCache, key: String) -> Option<LLMResponse> {
    match cache.cache.get(&key) {
        Some(response) => {
            cache.hit_count = cache.hit_count + 1;
            Some(response.clone())
        }
        None => {
            cache.miss_count = cache.miss_count + 1;
            None
        }
    }
}

pub fn cache_put(cache: &mut LLMCache, key: String, response: LLMResponse) {
    if cache.cache.len() >= cache.max_size as usize {
        // Simple eviction: remove first item
        if let Some(first_key) = cache.cache.keys().next().cloned() {
            cache.cache.remove(&first_key);
        }
    }
    cache.cache.insert(key, response);
}

pub fn get_cache_hit_rate(cache: &LLMCache) -> f32 {
    let total = cache.hit_count + cache.miss_count;
    if total == 0 {
        0.0
    } else {
        (cache.hit_count as f32) / (total as f32)
    }
}

// ═══════════════════════════════════════════════════════════════════════════
// Metrics Functions
// ═══════════════════════════════════════════════════════════════════════════

pub fn create_llm_metrics() -> LLMMetrics {
    LLMMetrics {
        total_requests: 0,
        successful_requests: 0,
        failed_requests: 0,
        average_response_time: 0,
        total_tokens_used: 0,
        cache_hit_rate: 0.0,
    }
}

pub fn update_metrics(
    metrics: &mut LLMMetrics,
    response: &LLMResponse,
) {
    metrics.total_requests = metrics.total_requests + 1;
    
    if response.success {
        metrics.successful_requests = metrics.successful_requests + 1;
        metrics.total_tokens_used = metrics.total_tokens_used + response.tokens_used;
        
        // Update average response time
        let total_time = metrics.average_response_time * (metrics.successful_requests - 1) + response.response_time;
        metrics.average_response_time = total_time / metrics.successful_requests;
    } else {
        metrics.failed_requests = metrics.failed_requests + 1;
    }
}

pub fn get_success_rate(metrics: &LLMMetrics) -> f32 {
    if metrics.total_requests == 0 {
        0.0
    } else {
        (metrics.successful_requests as f32) / (metrics.total_requests as f32)
    }
}

// ═══════════════════════════════════════════════════════════════════════════
// Validation Functions
// ═══════════════════════════════════════════════════════════════════════════

pub fn validate_config(config: &OllamaConfig) -> bool {
    !config.model_name.is_empty() &&
    !config.base_url.is_empty() &&
    config.timeout > 0 &&
    config.max_tokens > 0 &&
    config.temperature >= 0.0 &&
    config.temperature <= 2.0 &&
    config.top_p >= 0.0 &&
    config.top_p <= 1.0
}

pub fn validate_response(response: &LLMResponse) -> bool {
    if response.success {
        !response.text.is_empty() &&
        !response.model.is_empty() &&
        response.tokens_used > 0 &&
        response.response_time > 0
    } else {
        !response.error_message.is_empty()
    }
}

// ═══════════════════════════════════════════════════════════════════════════
// Utility Functions
// ═══════════════════════════════════════════════════════════════════════════

pub fn format_prompt(prompt: String, context: String) -> String {
    if context.is_empty() {
        prompt
    } else {
        format!("Context: {}\n\nPrompt: {}", context, prompt)
    }
}

pub fn truncate_response(response: String, max_length: i32) -> String {
    if response.len() > max_length as usize {
        response[0..max_length as usize].to_string()
    } else {
        response
    }
}

pub fn is_arabic_text(text: String) -> bool {
    text.chars().any(|c| {
        (c as u32) >= 0x0600 && (c as u32) <= 0x06FF
    })
}

pub fn detect_language(text: String) -> String {
    if is_arabic_text(text.clone()) {
        "arabic".to_string()
    } else {
        "english".to_string()
    }
}

// ═══════════════════════════════════════════════════════════════════════════
// End of LLM Wrapper Module
// ═══════════════════════════════════════════════════════════════════════════

