// std::ai::training - Training Module for AlBayan Language
// Expert recommendation: Priority 1 - High-level API for AI Training

module training;

// Import required modules
using std::ai::nn::{Sequential};
using std::ai::optimizers::{SGD, Adam, AdamW, RMSprop};
using std::ai::losses::{MSELoss, CrossEntropyLoss, BinaryCrossEntropyLoss, L1Loss};
using std::ai::{TorchTensorHandle, TorchModelHandle};

// External FFI declarations for training utilities
extern "C" {
    // Dataset utilities
    fn albayan_rt_dataset_create() -> usize;
    fn albayan_rt_dataset_add_sample(
        dataset_handle: usize,
        input: TorchTensorHandle,
        target: TorchTensorHandle
    );
    fn albayan_rt_dataset_get_batch(
        dataset_handle: usize,
        batch_size: i64,
        batch_index: i64
    ) -> usize; // Returns batch handle
    fn albayan_rt_dataset_size(dataset_handle: usize) -> i64;
    fn albayan_rt_batch_get_inputs(batch_handle: usize) -> TorchTensorHandle;
    fn albayan_rt_batch_get_targets(batch_handle: usize) -> TorchTensorHandle;
    
    // Training utilities
    fn albayan_rt_model_train_mode(model_handle: TorchModelHandle);
    fn albayan_rt_model_eval_mode(model_handle: TorchModelHandle);
    fn albayan_rt_tensor_item(tensor_handle: TorchTensorHandle) -> f64; // Get scalar value
}

// Dataset structure (Expert specification)
struct Dataset {
    handle: usize;
    size: i64;
}

impl Dataset {
    // Create new dataset
    fn new() -> Dataset {
        let handle = albayan_rt_dataset_create();
        Dataset {
            handle,
            size: 0,
        }
    }
    
    // Add sample to dataset
    fn add_sample(mut self, input: TorchTensorHandle, target: TorchTensorHandle) {
        albayan_rt_dataset_add_sample(self.handle, input, target);
        self.size += 1;
    }
    
    // Get dataset size
    fn size(self) -> i64 {
        albayan_rt_dataset_size(self.handle)
    }
    
    // Get batch
    fn get_batch(self, batch_size: i64, batch_index: i64) -> DataBatch {
        let batch_handle = albayan_rt_dataset_get_batch(self.handle, batch_size, batch_index);
        DataBatch { handle: batch_handle }
    }
    
    // Get number of batches
    fn num_batches(self, batch_size: i64) -> i64 {
        (self.size + batch_size - 1) / batch_size // Ceiling division
    }
}

// Data batch structure
struct DataBatch {
    handle: usize;
}

impl DataBatch {
    // Get batch inputs
    fn inputs(self) -> TorchTensorHandle {
        albayan_rt_batch_get_inputs(self.handle)
    }
    
    // Get batch targets
    fn targets(self) -> TorchTensorHandle {
        albayan_rt_batch_get_targets(self.handle)
    }
}

// Training configuration
struct TrainingConfig {
    epochs: i64;
    batch_size: i64;
    print_every: i64;
    validate_every: i64;
}

impl TrainingConfig {
    // Create default training config
    fn default() -> TrainingConfig {
        TrainingConfig {
            epochs: 10,
            batch_size: 32,
            print_every: 100,
            validate_every: 1,
        }
    }
    
    // Create custom training config
    fn new(epochs: i64, batch_size: i64, print_every: i64, validate_every: i64) -> TrainingConfig {
        TrainingConfig {
            epochs,
            batch_size,
            print_every,
            validate_every,
        }
    }
}

// Training results
struct TrainingResults {
    train_losses: List<f64>;
    val_losses: List<f64>;
    epochs_completed: i64;
}

impl TrainingResults {
    // Create new training results
    fn new() -> TrainingResults {
        TrainingResults {
            train_losses: [],
            val_losses: [],
            epochs_completed: 0,
        }
    }
    
    // Add training loss
    fn add_train_loss(mut self, loss: f64) {
        self.train_losses.push(loss);
    }
    
    // Add validation loss
    fn add_val_loss(mut self, loss: f64) {
        self.val_losses.push(loss);
    }
    
    // Get final training loss
    fn final_train_loss(self) -> f64 {
        if self.train_losses.len() > 0 {
            self.train_losses[self.train_losses.len() - 1]
        } else {
            0.0
        }
    }
    
    // Get final validation loss
    fn final_val_loss(self) -> f64 {
        if self.val_losses.len() > 0 {
            self.val_losses[self.val_losses.len() - 1]
        } else {
            0.0
        }
    }
}

// High-level training function (Expert specification)
fn train_model_sgd(
    model: Sequential,
    train_data: Dataset,
    val_data: Dataset,
    learning_rate: f64,
    config: TrainingConfig
) -> TrainingResults {
    // Create SGD optimizer
    let optimizer = SGD(model.handle, learning_rate);
    
    // Create MSE loss function
    let loss_fn = MSELoss::new();
    
    // Train the model
    train_with_optimizer_and_loss(model, train_data, val_data, optimizer, loss_fn, config)
}

// Training function with Adam optimizer
fn train_model_adam(
    model: Sequential,
    train_data: Dataset,
    val_data: Dataset,
    learning_rate: f64,
    config: TrainingConfig
) -> TrainingResults {
    // Create Adam optimizer
    let optimizer = Adam(model.handle, learning_rate);
    
    // Create MSE loss function
    let loss_fn = MSELoss::new();
    
    // Train the model
    train_with_optimizer_and_loss(model, train_data, val_data, optimizer, loss_fn, config)
}

// Generic training function (Expert specification)
fn train_with_optimizer_and_loss<O, L>(
    model: Sequential,
    train_data: Dataset,
    val_data: Dataset,
    optimizer: O,
    loss_fn: L,
    config: TrainingConfig
) -> TrainingResults {
    let mut results = TrainingResults::new();
    
    // Training loop
    for epoch in 0..config.epochs {
        // Set model to training mode
        albayan_rt_model_train_mode(model.handle);
        
        let mut epoch_loss = 0.0;
        let num_batches = train_data.num_batches(config.batch_size);
        
        // Batch training loop
        for batch_idx in 0..num_batches {
            // Get batch
            let batch = train_data.get_batch(config.batch_size, batch_idx);
            let inputs = batch.inputs();
            let targets = batch.targets();
            
            // Zero gradients
            optimizer.zero_grad();
            
            // Forward pass
            let predictions = model.forward(inputs);
            
            // Compute loss
            let loss = loss_fn.compute(predictions, targets);
            
            // Backward pass
            loss_fn.forward_backward(predictions, targets);
            
            // Optimizer step
            optimizer.step();
            
            // Accumulate loss
            epoch_loss += albayan_rt_tensor_item(loss);
            
            // Print progress
            if batch_idx % config.print_every == 0 {
                let current_loss = albayan_rt_tensor_item(loss);
                println!("Epoch {}/{}, Batch {}/{}, Loss: {:.6}", 
                    epoch + 1, config.epochs, batch_idx + 1, num_batches, current_loss);
            }
        }
        
        // Average epoch loss
        epoch_loss /= num_batches as f64;
        results.add_train_loss(epoch_loss);
        
        // Validation
        if epoch % config.validate_every == 0 {
            let val_loss = validate_model(model, val_data, loss_fn, config.batch_size);
            results.add_val_loss(val_loss);
            
            println!("Epoch {}/{} - Train Loss: {:.6}, Val Loss: {:.6}", 
                epoch + 1, config.epochs, epoch_loss, val_loss);
        }
        
        results.epochs_completed = epoch + 1;
    }
    
    results
}

// Validation function
fn validate_model<L>(
    model: Sequential,
    val_data: Dataset,
    loss_fn: L,
    batch_size: i64
) -> f64 {
    // Set model to evaluation mode
    albayan_rt_model_eval_mode(model.handle);
    
    let mut total_loss = 0.0;
    let num_batches = val_data.num_batches(batch_size);
    
    // Validation loop
    for batch_idx in 0..num_batches {
        let batch = val_data.get_batch(batch_size, batch_idx);
        let inputs = batch.inputs();
        let targets = batch.targets();
        
        // Forward pass (no gradients)
        let predictions = model.forward(inputs);
        
        // Compute loss
        let loss = loss_fn.compute(predictions, targets);
        total_loss += albayan_rt_tensor_item(loss);
    }
    
    // Return average validation loss
    total_loss / num_batches as f64
}

// Public API functions (Expert recommendation)

// Simple training function with default parameters
fn train(
    model: Sequential,
    train_data: Dataset,
    val_data: Dataset,
    learning_rate: f64,
    epochs: i64
) -> TrainingResults {
    let config = TrainingConfig::new(epochs, 32, 100, 1);
    train_model_adam(model, train_data, val_data, learning_rate, config)
}

// Create dataset
fn create_dataset() -> Dataset {
    Dataset::new()
}

// Create training configuration
fn training_config(epochs: i64, batch_size: i64) -> TrainingConfig {
    TrainingConfig::new(epochs, batch_size, 100, 1)
}
